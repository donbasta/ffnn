{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    possible_val = []\n",
    "    for val in y_true:\n",
    "        if val not in possible_val:\n",
    "            possible_val.append(val)\n",
    "\n",
    "    for val in y_pred:\n",
    "      if val not in possible_val:\n",
    "        possible_val.append(val)\n",
    "\n",
    "    dic = {}\n",
    "    possible_val.sort()\n",
    "    for i in range(len(possible_val)):\n",
    "        dic[possible_val[i]] = i\n",
    "\n",
    "    mat = [[0 for j in range(len(possible_val))]\n",
    "           for i in range(len(possible_val))]\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        mat[dic[y_true[i]]][dic[y_pred[i]]] += 1\n",
    "\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify(data, index, columns):\n",
    "    ret = DataFrame(data)\n",
    "    ret.index = index\n",
    "    ret.columns = columns\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        # self.y_true = y_true\n",
    "        # self.y_pred = y_pred\n",
    "        self.confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "        self.labels = []\n",
    "        for val in y_true:\n",
    "            if val not in self.labels:\n",
    "                self.labels.append(val)\n",
    "\n",
    "        for val in y_pred:\n",
    "            if val not in self.labels:\n",
    "                self.labels.append(val) \n",
    "\n",
    "        self.labels.sort()\n",
    "\n",
    "    def get_label_tp_tn_fp_fn(self, label):\n",
    "        tp = self.confusion_matrix[label][label]\n",
    "        tn, fp, fn = 0, 0, 0\n",
    "        n = len(self.confusion_matrix)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                val = self.confusion_matrix[i][j]\n",
    "                if (i != label and j != label):\n",
    "                    tn += val\n",
    "                if (j == label and i != label):\n",
    "                    fp += val\n",
    "                if (i == label and j != label):\n",
    "                    fn += val\n",
    "        return tp, tn, fp, fn\n",
    "\n",
    "    def overall_accuracy(self):\n",
    "        tp = 0\n",
    "        tot = 0\n",
    "        n = len(self.confusion_matrix)\n",
    "        for i in range(n):\n",
    "            tp += self.confusion_matrix[i][i]\n",
    "            for j in range(n):\n",
    "                tot += self.confusion_matrix[i][j]\n",
    "        return tp / tot\n",
    "\n",
    "    # label yg dimaksud itu indeks labelnya\n",
    "    # label = [\"a\", \"b\", \"c\"]\n",
    "    # kalo mau accuracy \"a\" manggil self.accuracy(0)\n",
    "    def accuracy(self, label):\n",
    "        tp, tn, fp, fn = self.get_label_tp_tn_fp_fn(label)\n",
    "        if(tp + tn == 0):\n",
    "            return 0\n",
    "        return (tp + tn) / (tp + tn + fn + fp)\n",
    "\n",
    "    def all_accuracy(self):\n",
    "        n = len(self.confusion_matrix)\n",
    "        tot = 0\n",
    "        for i in range(n):\n",
    "            tot += self.accuracy(i)\n",
    "        \n",
    "        if(tot == 0):\n",
    "            return 0\n",
    "        return tot / n\n",
    "\n",
    "    def precision(self, label):\n",
    "        tp, tn, fp, fn = self.get_label_tp_tn_fp_fn(label)\n",
    "        if(tp == 0):\n",
    "            return 0\n",
    "        return tp / (tp + fp)\n",
    "    \n",
    "    def all_precision(self):\n",
    "        n = len(self.confusion_matrix)\n",
    "        tot = 0\n",
    "        for i in range(n):\n",
    "            tot += self.precision(i)\n",
    "        if(tot == 0):\n",
    "            return 0\n",
    "        return tot / n\n",
    "\n",
    "    def recall(self, label):\n",
    "        tp, tn, fp, fn = self.get_label_tp_tn_fp_fn(label)\n",
    "        if(tp == 0):\n",
    "            return 0\n",
    "        return tp / (tp + fn)\n",
    "\n",
    "    def all_recall(self):\n",
    "        n = len(self.confusion_matrix)\n",
    "        tot = 0\n",
    "        for i in range(n):\n",
    "            tot += self.recall(i)\n",
    "        if(tot == 0):\n",
    "            return 0\n",
    "        return tot / n\n",
    "\n",
    "    def f1_score(self, label):\n",
    "        tp, tn, fp, fn = self.get_label_tp_tn_fp_fn(label)\n",
    "        precision = self.precision(label)\n",
    "        recall = self.recall(label)\n",
    "        if(precision * recall == 0):\n",
    "            return 0\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    def all_f1_score(self):\n",
    "        n = len(self.confusion_matrix)\n",
    "        tot = 0\n",
    "        for i in range(n):\n",
    "            tot += self.f1_score(i)\n",
    "        if(tot == 0):\n",
    "            return 0\n",
    "        return tot / n\n",
    "\n",
    "    def report(self, digits = 3):\n",
    "        # accuracy, precision, recall, f1\n",
    "        n = len(self.confusion_matrix)\n",
    "        ret = [[0 for j in range(4)] for i in range(n)]\n",
    "        for i in range(n):\n",
    "            ret[i][0] = round(self.accuracy(i), digits)\n",
    "            ret[i][1] = round(self.precision(i), digits)\n",
    "            ret[i][2] = round(self.recall(i), digits)\n",
    "            ret[i][3] = round(self.f1_score(i), digits)\n",
    "        \n",
    "        print(prettify(ret, self.labels, [\"accuracy\", \"precision\", \"recall\", \"f1\"]))\n",
    "        print(f'overall accuracy: {self.all_accuracy():.3f}')\n",
    "        print(f'overall precision: {self.all_precision():.3f}')\n",
    "        print(f'overall recall: {self.all_recall():.3f}')\n",
    "        print(f'overall f1_score: {self.all_f1_score():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # applying the sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # computing derivative to the Sigmoid function\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    # compute relu\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return np.full(x.shape, 1)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    ret = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        ex = np.exp(x[:, i])\n",
    "        x[:, i] = ex / np.sum(ex)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    ret = np.zeros(x.shape);\n",
    "    for i in range(x.shape[1]):\n",
    "        j = np.sum(softmax_derivative_util(x[:, i]), axis=1)\n",
    "        ret[:, i] = j\n",
    "    return ret\n",
    "\n",
    "\n",
    "def softmax_derivative_util(x):\n",
    "    rx = x.reshape(-1, 1)\n",
    "    return np.diagflat(rx) - np.dot(rx, rx.T)\n",
    "\n",
    "\n",
    "def sum_of_squared_error(t, o):\n",
    "    sub = t - o\n",
    "    return 0.5 * np.sum(sub**2)\n",
    "\n",
    "\n",
    "def cross_entropy(t, o):\n",
    "    ret = 0\n",
    "    for i in range(o.shape[1]):\n",
    "        j = np.argmax(o[:, i])\n",
    "        ct = t[j, i]\n",
    "        # ct = clip_scalar(t[j, i])\n",
    "        ret += -np.log2(ct)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def cross_entropy_derivative(t, o):\n",
    "    ret = o\n",
    "    for i in range(o.shape[1]):\n",
    "        j = np.argmax(o[:, i])\n",
    "        ret[j, i] = -(1-ret[j, i])\n",
    "    return ret\n",
    "\n",
    "\n",
    "clip_upper_threshold = 5\n",
    "clip_lower_threshold = 0.5\n",
    "def clip(x):\n",
    "    ret = x\n",
    "    norm = np.sum(x * x)\n",
    "    if norm > clip_upper_threshold ** 2:\n",
    "        ret = ret * (clip_upper_threshold / np.sqrt(norm))\n",
    "#     if norm < clip_lower_threshold ** 2:\n",
    "#         ret = ret * (clip_lower_threshold / np.sqrt(norm))\n",
    "    return ret\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    # activation_functions\n",
    "    \"relu\": relu,\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"softmax\": softmax,\n",
    "    \"linear\": linear,\n",
    "}\n",
    "\n",
    "activation_functions_derivative = {\n",
    "    # activation_functions_derivative\n",
    "    \"relu\": relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "    \"softmax\": softmax_derivative,\n",
    "    \"linear\": linear_derivative\n",
    "}\n",
    "\n",
    "error_functions = {\n",
    "    # error_functions\n",
    "    \"relu\": sum_of_squared_error,\n",
    "    \"sigmoid\": sum_of_squared_error,\n",
    "    \"softmax\": cross_entropy,\n",
    "    \"linear\": sum_of_squared_error,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, activation, input, output):\n",
    "        self.activation_name = activation\n",
    "        self.activation = activation_functions[activation]\n",
    "        self.cost_function = error_functions[activation]\n",
    "        self.activation_derivative = activation_functions_derivative[activation]\n",
    "        self.W = np.random.randn(output, input)\n",
    "        self.b = np.random.randn(output, 1)\n",
    "        \n",
    "        self.reset_delta(output)\n",
    "        self.reset_delta_weight()\n",
    "        self.reset_delta_bias()\n",
    "        self.output = np.zeros(output)\n",
    "        self.net = np.zeros(output)\n",
    "    \n",
    "    def set_delta(self, delta):\n",
    "        self.delta = delta\n",
    "    \n",
    "    def set_weight(self, weight):\n",
    "        self.W = weight\n",
    "\n",
    "    def set_bias(self, bias):\n",
    "        self.b = bias\n",
    "\n",
    "    def add_delta_weight(self, delta):\n",
    "        self.delta_weight += delta\n",
    "\n",
    "    def add_delta_bias(self, delta_bias):\n",
    "        self.delta_bias += delta_bias\n",
    "    \n",
    "    def reset_delta(self, output):\n",
    "        self.delta = np.zeros(output)\n",
    "\n",
    "    def reset_delta_weight(self):\n",
    "        self.delta_weight = np.zeros(self.W.shape)\n",
    "\n",
    "    def reset_delta_bias(self):\n",
    "        self.delta_bias = np.zeros(self.b.shape)\n",
    "\n",
    "    def set_output(self, output):\n",
    "        self.output = output\n",
    "\n",
    "    def set_net(self, net):\n",
    "        self.net = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate=0.05, max_iter=500, error_threshold=0.01, batch_size=5, verbose=False):\n",
    "        # seeding for random number generation\n",
    "        np.random.seed(1)\n",
    "        self.layers = []\n",
    "        self.depth = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.error_threshold = error_threshold\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.depth += 1\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def init_layers(self, layer_description):\n",
    "        for a in layer_description:\n",
    "            layer = self.Layer(a.activation_type,\n",
    "                               a.previous_neuron, a.current_neuron)\n",
    "            self.add(layer)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        f = open(filename, \"r\")\n",
    "\n",
    "        self.depth = int(f.readline())\n",
    "\n",
    "        for i in range(self.depth):\n",
    "\n",
    "            n_neuron = int(f.readline())\n",
    "            activation_type = f.readline()[:-1]\n",
    "            weight = []\n",
    "\n",
    "            n_neuron_prev = -1\n",
    "            for j in range(n_neuron):\n",
    "                temp = list(map(float, f.readline().split()))\n",
    "                weight.append(temp)\n",
    "                if (n_neuron_prev == -1):\n",
    "                    n_neuron_prev = len(temp)\n",
    "\n",
    "            layer = Layer(activation_type, n_neuron_prev, n_neuron)\n",
    "            layer.set_weight(np.array(weight))\n",
    "            bias = np.array(\n",
    "                list(map(lambda x: [float(x)],\n",
    "                         f.readline().split())))\n",
    "            layer.set_bias(bias)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        f = open(filename, \"w\")\n",
    "\n",
    "        f.write(str(self.depth) + \"\\n\")\n",
    "        for layer in self.layers:\n",
    "            n_neuron = len(layer.b)\n",
    "            f.write(str(n_neuron) + \"\\n\")\n",
    "            f.write(layer.activation_name + \"\\n\")\n",
    "            for i in range(n_neuron):\n",
    "                f.write(\" \".join(list(map(str, layer.W[i]))) + \"\\n\")\n",
    "            f.write(\" \".join(list(map(lambda x: str(x[0]), layer.b))) + \"\\n\")\n",
    "\n",
    "    def forward_propagate(self, x_inputs):\n",
    "        a = np.array(x_inputs).T\n",
    "        for layer in self.layers:\n",
    "            z = np.dot(layer.W, a) + layer.b\n",
    "            layer.set_net(z)\n",
    "            a = layer.activation(z)\n",
    "            layer.set_output(a)\n",
    "        return a\n",
    "\n",
    "    def backward_propagate(self, X_train, y_train, prediction):\n",
    "        grad = {}\n",
    "\n",
    "        num_layers = len(self.layers)\n",
    "        \n",
    "        for i in reversed(range(num_layers)):\n",
    "            layer = self.layers[i]\n",
    "            \n",
    "            # if output layer\n",
    "            if i == num_layers - 1:\n",
    "                # use squared error derivative if not softmax\n",
    "                if self.layers[-1].activation != 'softmax':\n",
    "                    layer.delta = clip((prediction - y_train) * layer.activation_derivative(layer.net))\n",
    "                else:\n",
    "                    layer.delta = cross_entropy_derivative(prediction, y_train) * layer.activation_derivative(layer.net)\n",
    "            else:\n",
    "                next_layer = self.layers[i + 1]\n",
    "                error = np.dot(next_layer.W.T, next_layer.delta)\n",
    "                layer.delta = clip(error * layer.activation_derivative(layer.net))\n",
    "            \n",
    "        for i in range(num_layers):\n",
    "            layer = self.layers[i]\n",
    "            input_activation = np.atleast_2d(X_train if i == 0 else self.layers[i - 1].output)\n",
    "            grad[\"dW\" + str(i)] = clip(np.dot(layer.delta, input_activation.T) * self.learning_rate)\n",
    "            grad[\"db\" + str(i)] = clip(layer.delta * self.learning_rate)\n",
    "            \n",
    "        return grad\n",
    "\n",
    "    def shuffle(self, x_train, y_train):\n",
    "        sz = len(y_train)\n",
    "        ids = [i for i in range(sz)]\n",
    "        np.random.shuffle(ids)\n",
    "        ret_x, ret_y = [], []\n",
    "        for i in ids:\n",
    "            ret_x.append(list(x_train[i]))\n",
    "            ret_y.append(list(y_train[i]))\n",
    "        return (ret_x), (ret_y)\n",
    "\n",
    "    def split_batch(self, x_train, y_train):\n",
    "        batches_x = []\n",
    "        batches_y = []\n",
    "\n",
    "        length = len(x_train)\n",
    "\n",
    "        for i in range((length // self.batch_size)):\n",
    "            x_batch = x_train[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            y_batch = y_train[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batches_x.append(np.array(x_batch))\n",
    "            batches_y.append(np.array(y_batch))\n",
    "        if length % self.batch_size != 0:\n",
    "            i = length // self.batch_size\n",
    "            x_batch = x_train[i * self.batch_size:]\n",
    "            y_batch = y_train[i * self.batch_size:]\n",
    "            batches_x.append(np.array(x_batch))\n",
    "            batches_y.append(np.array(y_batch))\n",
    "\n",
    "        return (batches_x), (batches_y)\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for iteration in range(self.max_iter):\n",
    "\n",
    "            x_train, y_train = self.shuffle(x_train, y_train)\n",
    "\n",
    "            batches_x, batches_y = self.split_batch(x_train, y_train)\n",
    "\n",
    "            cost_function = 0\n",
    "            for i in range(len(batches_x)):\n",
    "                x_input = batches_x[i]\n",
    "                y_output = batches_y[i]\n",
    "\n",
    "                prediction = self.forward_propagate(x_input)\n",
    "                cost_function += self.layers[-1].cost_function(\n",
    "                    prediction, y_output.T)\n",
    "                gradients = self.backward_propagate(\n",
    "                    x_input.T, y_output.T, prediction)\n",
    "                \n",
    "                # update delta phase\n",
    "                for j, layer in enumerate(self.layers):\n",
    "                    layer.add_delta_weight(gradients[\"dW\" + str(j)])\n",
    "                    grad_bias = gradients[\"db\" + str(j)]\n",
    "                    layer.add_delta_bias(np.sum(grad_bias, axis=1).reshape(len(grad_bias), 1))\n",
    "                \n",
    "                # update weights phase\n",
    "                for j, layer in enumerate(self.layers):\n",
    "                    layer.W += layer.delta_weight  # gradients[\"dW\" + str(j)]\n",
    "                    layer.b += layer.delta_bias  # gradients[\"db\" + str(j)]\n",
    "                \n",
    "                for j, layer in enumerate(self.layers):\n",
    "                    layer.reset_delta_weight()\n",
    "                    layer.reset_delta_bias()\n",
    "            \n",
    "            cost_function /= len(x_train)\n",
    "            if iteration % 50 == 0:\n",
    "                print(f\"Iteration {iteration}: \", cost_function)\n",
    "\n",
    "            if cost_function < self.error_threshold:\n",
    "                break\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        prediction = self.forward_propagate(x_test)\n",
    "        return prediction\n",
    "\n",
    "    def __str__(self):\n",
    "        index = 1\n",
    "        res = \"\"\n",
    "        for layer in self.layers:\n",
    "            res += \"{}-th layer\\n\".format(index)\n",
    "            res += f\"Activation: {layer.activation_name}\\n\"\n",
    "            res += \"Weight matrix:\\n\"\n",
    "            res += layer.W.__str__() + \"\\n\"\n",
    "            res += \"Bias\\n\"\n",
    "            res += layer.b.__str__() + \"\\n\\n\"\n",
    "            index += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:  0.3183902492549609\n",
      "Iteration 50:  0.04764504032348603\n",
      "Iteration 100:  0.03805905068935214\n",
      "Iteration 150:  0.02053510845599345\n",
      "Iteration 200:  0.03784032822775462\n",
      "Iteration 250:  0.024026457059622326\n",
      "Iteration 300:  0.026548040779272957\n",
      "Iteration 350:  0.02422937261430657\n",
      "Iteration 400:  0.029953753766413658\n",
      "Iteration 450:  0.03782240507182985\n",
      "Iteration 500:  0.015999987520767332\n",
      "Iteration 550:  0.02800142893615686\n",
      "Iteration 600:  0.025151904103365683\n",
      "ACCURACY SLURRR:  1.0\n",
      "PRECISION SLURRR:  1.0\n",
      "RECALL SLURRR:  1.0\n",
      "F1 SLURRR:  1.0\n",
      "CONFUSION MATRIXX: \n",
      "[[6, 0, 0], [0, 5, 0], [0, 0, 4]]\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#train-test-split 90%-10%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42069)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# create model\n",
    "model = NeuralNetwork(learning_rate=0.001, max_iter=2000, verbose=False)\n",
    "model.add(Layer(\"relu\", 4, 10))\n",
    "model.add(Layer(\"relu\", 10, 10))\n",
    "model.add(Layer(\"linear\", 10, 5))\n",
    "model.add(Layer(\"sigmoid\", 5, 3))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#bikin confusion matrix dan metric dari training ini:\n",
    "prediction = model.predict(X_test)\n",
    "label_pred = []\n",
    "for i in range(prediction.shape[1]):\n",
    "    label_pred.append(np.argmax(prediction[:, i]))\n",
    "y_test_label = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_label.append(np.argmax(y_test[i, :]))\n",
    "metrics = Metrics(y_test_label, label_pred)\n",
    "print(\"ACCURACY: \", metrics.all_accuracy())\n",
    "print(\"PRECISION: \", metrics.all_precision())\n",
    "print(\"RECALL: \", metrics.all_recall())\n",
    "print(\"F1: \", metrics.all_f1_score())\n",
    "print(\"CONFUSION MATRIX: \")\n",
    "print(confusion_matrix(y_test_label, label_pred))\n",
    "\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n",
      "TRAIN DATA INDEX\n",
      "[ 15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32\n",
      "  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Iteration 0:  0.39937328880962825\n",
      "Iteration 50:  0.04978689816065027\n",
      "Iteration 100:  0.046302060608163095\n",
      "Iteration 150:  0.037639947227490186\n",
      "Iteration 200:  0.03416496216213545\n",
      "Iteration 250:  0.04312085493846274\n",
      "Iteration 300:  0.03329399228431469\n",
      "Iteration 350:  0.03093125061546607\n",
      "Iteration 400:  0.014304452140782633\n",
      "Iteration 450:  0.029751889241869937\n",
      "Iteration 500:  0.021201205326134735\n",
      "Iteration 550:  0.039217012901519105\n",
      "Iteration 600:  0.025280085328910466\n",
      "Iteration 650:  0.0335931204038605\n",
      "Iteration 700:  0.024661866496472253\n",
      "Iteration 750:  0.04610135821713308\n",
      "Iteration 800:  0.026617263125413067\n",
      "   accuracy  precision  recall   f1\n",
      "0       1.0        1.0     1.0  1.0\n",
      "overall accuracy: 1.000\n",
      "overall precision: 1.000\n",
      "overall recall: 1.000\n",
      "overall f1_score: 1.000\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  30  31  32\n",
      "  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "Iteration 0:  0.3906122588872123\n",
      "Iteration 50:  0.040330498972528175\n",
      "Iteration 100:  0.044265798311197424\n",
      "Iteration 150:  0.04761492490747132\n",
      "Iteration 200:  0.04008368427565766\n",
      "Iteration 250:  0.042881030116223316\n",
      "Iteration 300:  0.01802330962363282\n",
      "Iteration 350:  0.035884799111425024\n",
      "Iteration 400:  0.013495687794104781\n",
      "Iteration 450:  0.030725500672805488\n",
      "Iteration 500:  0.024275915683943446\n",
      "Iteration 550:  0.03772128805073547\n",
      "Iteration 600:  0.02961105748346587\n",
      "Iteration 650:  0.03065620876581967\n",
      "Iteration 700:  0.03490447961261477\n",
      "Iteration 750:  0.03740345364270223\n",
      "Iteration 800:  0.030301807033795536\n",
      "   accuracy  precision  recall   f1\n",
      "0       1.0        1.0     1.0  1.0\n",
      "overall accuracy: 1.000\n",
      "overall precision: 1.000\n",
      "overall recall: 1.000\n",
      "overall f1_score: 1.000\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  45  46  47  48  49  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n",
      "Iteration 0:  0.4171566821647122\n",
      "Iteration 50:  0.05121232113920098\n",
      "Iteration 100:  0.04832819610607747\n",
      "Iteration 150:  0.03932093651271849\n",
      "Iteration 200:  0.037864136865288625\n",
      "Iteration 250:  0.04205261687157766\n",
      "Iteration 300:  0.0260725463678152\n",
      "Iteration 350:  0.031901396754422794\n",
      "Iteration 400:  0.014496844039040066\n",
      "Iteration 450:  0.03231508969632846\n",
      "Iteration 500:  0.01978443166479547\n",
      "Iteration 550:  0.0336957246221723\n",
      "Iteration 600:  0.026642502208786164\n",
      "Iteration 650:  0.03401433752221281\n",
      "Iteration 700:  0.04730733574523879\n",
      "Iteration 750:  0.038297356028911926\n",
      "Iteration 800:  0.03167615270351398\n",
      "   accuracy  precision  recall   f1\n",
      "0       1.0        1.0     1.0  1.0\n",
      "overall accuracy: 1.000\n",
      "overall precision: 1.000\n",
      "overall recall: 1.000\n",
      "overall f1_score: 1.000\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  60  61  62  63  64  65  66  67  68\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[45 46 47 48 49 50 51 52 53 54 55 56 57 58 59]\n",
      "Iteration 0:  0.3448949921806437\n",
      "Iteration 50:  0.048575447258842726\n",
      "Iteration 100:  0.04561748098143514\n",
      "Iteration 150:  0.03880669683318288\n",
      "Iteration 200:  0.04189337252801134\n",
      "Iteration 250:  0.04251926218956849\n",
      "Iteration 300:  0.026009144990873336\n",
      "Iteration 350:  0.036587074617222076\n",
      "Iteration 400:  0.014079026714813469\n",
      "Iteration 450:  0.033456479329546956\n",
      "Iteration 500:  0.02307831874421066\n",
      "Iteration 550:  0.038271425585731884\n",
      "Iteration 600:  0.02878933652485788\n",
      "Iteration 650:  0.03147011551925716\n",
      "Iteration 700:  0.024868763593859873\n",
      "Iteration 750:  0.043971287664432875\n",
      "Iteration 800:  0.03013056461758563\n",
      "   accuracy  precision  recall   f1\n",
      "0       1.0        1.0     1.0  1.0\n",
      "1       1.0        1.0     1.0  1.0\n",
      "overall accuracy: 1.000\n",
      "overall precision: 1.000\n",
      "overall recall: 1.000\n",
      "overall f1_score: 1.000\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74]\n",
      "Iteration 0:  0.3303470341475604\n",
      "Iteration 50:  0.04490973512656807\n",
      "   accuracy  precision  recall     f1\n",
      "1     0.933        1.0   0.933  0.966\n",
      "2     0.933        0.0   0.000  0.000\n",
      "overall accuracy: 0.933\n",
      "overall precision: 0.500\n",
      "overall recall: 0.467\n",
      "overall f1_score: 0.483\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "Iteration 0:  0.34615210216737174\n",
      "Iteration 50:  0.03862184379370044\n",
      "   accuracy  precision  recall     f1\n",
      "1       0.8        1.0     0.8  0.889\n",
      "2       0.8        0.0     0.0  0.000\n",
      "overall accuracy: 0.800\n",
      "overall precision: 0.500\n",
      "overall recall: 0.400\n",
      "overall f1_score: 0.444\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "Iteration 0:  0.34249223341965745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50:  0.04407883744396616\n",
      "Iteration 100:  0.035251149574306265\n",
      "Iteration 150:  0.03252114344034898\n",
      "Iteration 200:  0.03289340710227642\n",
      "Iteration 250:  0.03656951104281978\n",
      "Iteration 300:  0.026352208609194655\n",
      "Iteration 350:  0.033658939017660504\n",
      "Iteration 400:  0.03238711174262154\n",
      "Iteration 450:  0.019992134207046826\n",
      "Iteration 500:  0.025011538210929787\n",
      "Iteration 550:  0.02344106821867751\n",
      "Iteration 600:  0.02679920139042679\n",
      "Iteration 650:  0.027429468616654483\n",
      "Iteration 700:  0.014740286334608037\n",
      "Iteration 750:  0.025197407007096224\n",
      "Iteration 800:  0.037622560438355906\n",
      "Iteration 850:  0.021052006821737534\n",
      "Iteration 900:  0.025914507895461514\n",
      "Iteration 950:  0.03271751004548546\n",
      "   accuracy  precision  recall   f1\n",
      "1       1.0        1.0     1.0  1.0\n",
      "2       1.0        1.0     1.0  1.0\n",
      "overall accuracy: 1.000\n",
      "overall precision: 1.000\n",
      "overall recall: 1.000\n",
      "overall f1_score: 1.000\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[105 106 107 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "Iteration 0:  0.34273233128616604\n",
      "Iteration 50:  0.03210394959736271\n",
      "Iteration 100:  0.03617168908211247\n",
      "Iteration 150:  0.025587564788053168\n",
      "Iteration 200:  0.032702259961308947\n",
      "Iteration 250:  0.03612408645466872\n",
      "Iteration 300:  0.020956489714018534\n",
      "Iteration 350:  0.02789720217492909\n",
      "Iteration 400:  0.03262911422713517\n",
      "Iteration 450:  0.022201179857161333\n",
      "Iteration 500:  0.030214560535601773\n",
      "Iteration 550:  0.023574650407387824\n",
      "Iteration 600:  0.03319796112084177\n",
      "Iteration 650:  0.025197663929691045\n",
      "Iteration 700:  0.021627060702647763\n",
      "Iteration 750:  0.02607886894287598\n",
      "Iteration 800:  0.023519971831936205\n",
      "Iteration 850:  0.020267720438139687\n",
      "Iteration 900:  0.02264894685418159\n",
      "Iteration 950:  0.02064012475387959\n",
      "   accuracy  precision  recall   f1\n",
      "2       1.0        1.0     1.0  1.0\n",
      "overall accuracy: 1.000\n",
      "overall precision: 1.000\n",
      "overall recall: 1.000\n",
      "overall f1_score: 1.000\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149]\n",
      "TEST DATA INDEX\n",
      "[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134]\n",
      "Iteration 0:  0.3430396268693301\n",
      "Iteration 50:  0.020770697177730887\n",
      "Iteration 100:  0.01662455867042806\n",
      "Iteration 150:  0.009862355883482698\n",
      "   accuracy  precision  recall     f1\n",
      "1     0.467        0.0   0.000  0.000\n",
      "2     0.467        1.0   0.467  0.636\n",
      "overall accuracy: 0.467\n",
      "overall precision: 0.500\n",
      "overall recall: 0.233\n",
      "overall f1_score: 0.318\n",
      "TRAIN DATA INDEX\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134]\n",
      "TEST DATA INDEX\n",
      "[135 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "Iteration 0:  0.3381399813716052\n",
      "Iteration 50:  0.030400819974985293\n",
      "Iteration 100:  0.02191116753617455\n",
      "Iteration 150:  0.03078450054761991\n",
      "Iteration 200:  0.025219806448061047\n",
      "Iteration 250:  0.03177964842412813\n",
      "Iteration 300:  0.021684666575930916\n",
      "Iteration 350:  0.01600712722919134\n",
      "Iteration 400:  0.022201802141446788\n",
      "Iteration 450:  0.021056863404572662\n",
      "Iteration 500:  0.01625162528651977\n",
      "Iteration 550:  0.0245440473018561\n",
      "Iteration 600:  0.015486039849931318\n",
      "Iteration 650:  0.023483799797568046\n",
      "Iteration 700:  0.022403023733162112\n",
      "Iteration 750:  0.022751337962156736\n",
      "Iteration 800:  0.01703452544700577\n",
      "Iteration 850:  0.02686978855382639\n",
      "Iteration 900:  0.023782512512942378\n",
      "Iteration 950:  0.027000018532561898\n",
      "   accuracy  precision  recall   f1\n",
      "2       1.0        1.0     1.0  1.0\n",
      "overall accuracy: 1.000\n",
      "overall precision: 1.000\n",
      "overall recall: 1.000\n",
      "overall f1_score: 1.000\n"
     ]
    }
   ],
   "source": [
    "#10-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k_fold = KFold(n_splits=10)\n",
    "print(k_fold)\n",
    "scores = []\n",
    "for train_index, test_index in k_fold.split(X):\n",
    "    print(\"TRAIN DATA INDEX\")\n",
    "    print(train_index)\n",
    "    print(\"TEST DATA INDEX\")\n",
    "    print(test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model_tmp = NeuralNetwork(learning_rate=0.001, max_iter=1000, verbose=False)\n",
    "    model_tmp.add(Layer(\"relu\", 4, 10))\n",
    "    model_tmp.add(Layer(\"relu\", 10, 10))\n",
    "    model_tmp.add(Layer(\"linear\", 10, 5))\n",
    "    model_tmp.add(Layer(\"sigmoid\", 5, 3))\n",
    "    model_tmp.fit(X_train, y_train)\n",
    "    prediction = model_tmp.predict(X_test)\n",
    "    label_pred = []\n",
    "    for i in range(prediction.shape[1]):\n",
    "        label_pred.append(np.argmax(prediction[:, i]))\n",
    "    y_test_label = []\n",
    "    for i in range(y_test.shape[0]):\n",
    "        y_test_label.append(np.argmax(y_test[i, :]))\n",
    "    metrics = Metrics(y_test_label, label_pred)\n",
    "    metrics.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Simpan model\n",
    "model_filename = \"model.txt\"\n",
    "model.save_model(model_filename)\n",
    "\n",
    "# Load model yang baru disimpan\n",
    "loaded_model = NeuralNetwork(learning_rate=0.001, max_iter=2000, verbose=False)\n",
    "loaded_model.load_model(model_filename)\n",
    "\n",
    "# Bikin instance data baru, predict pake model yg di-load\n",
    "instances = [\n",
    "  [6.9, 3.2, 4.7, 1.4], # versicolor (1)\n",
    "  [5.0, 3.5, 1.4, 0.2], # setosa (0)\n",
    "  [6.3, 3.3, 6.0, 2.4], # virginica (2)\n",
    "]\n",
    "result = loaded_model.forward_propagate(instances)\n",
    "print(list(map(np.argmax, result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis dari 2 hal ini: \n",
    "# 2. Lakukan pengujian dengan membandingkan confusion matrix dan perhitungan kinerja\n",
    "# dari sklearn.\n",
    "# 3. Lakukan pembelajaran FFNN untuk dataset iris dengan skema split train 90% dan\n",
    "# test 10%, dan menampilkan kinerja serta confusion matrixnya.\n",
    "# berdasarkan hasil yang sudah kami jalankan untuk skema split train 90% dan\n",
    "# test 10%, model yang didapatkan sudah cukup akurat dan ini dapat dilihat\n",
    "# dari hasil accuracy, precision, recall, dan F1nya. begitu juga confusion\n",
    "# matrixnya yang tidak ada persebaran selain di cell yang tepat prediksi\n",
    "# dan aslinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
